{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba19bcd",
   "metadata": {},
   "source": [
    "# Lane Detection using Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f4323",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this notebook, we perform Lane Detection by fine tuning a Vision Transformer Model on the OpenLaneV2 dataset. This task is essential for many downstream tasks like Bird's Eye View Representation, Motion Planning, 3D Occupancy Detection etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8327e",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The OpenLane dataset consists of multi-view images taken by a cameras mounted on an ego vehicle. The dataset provides annotations for Lane lines, Traffic elements,Area elements(Pedestrian Crossing, Sidewalk etc)\n",
    "\n",
    "Dataset Link - [here](https://github.com/OpenDriveLab/OpenLane-V2/blob/master/data/README.md#download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0b521",
   "metadata": {},
   "source": [
    "### RTDETR for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bae48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8bbfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sofa: 0.97 [0.14, 0.38, 640.13, 476.21]\n",
      "cat: 0.96 [343.38, 24.28, 640.14, 371.5]\n",
      "cat: 0.96 [13.23, 54.18, 318.98, 472.22]\n",
      "remote: 0.95 [40.11, 73.44, 175.96, 118.48]\n",
      "remote: 0.92 [333.73, 76.58, 369.97, 186.99]\n"
     ]
    }
   ],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg' \n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\n",
    "model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\n",
    "\n",
    "for result in results:\n",
    "    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "        score, label = score.item(), label_id.item()\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0c672",
   "metadata": {},
   "source": [
    "### ResNet for Laneline Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e033fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950a692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dict_subset_A = pd.read_pickle('../repo/OpenLane-V2/data/OpenLane-V2/data_dict_subset_A.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d7c7eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict_train = {}\n",
    "data_dict_val = {}\n",
    "\n",
    "for i in data_dict_subset_A.keys():\n",
    "    if i[0] == 'train':\n",
    "        data_dict_train[i] = data_dict_subset_A[i]\n",
    "    if i[0] == 'val':\n",
    "        data_dict_val[i] = data_dict_subset_A[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fe617",
   "metadata": {},
   "source": [
    "### Project 3d to 2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655fec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Binaries and/or source for the following packages or projects \n",
    "# are presented under one or more of the following open source licenses:\n",
    "# utils.py    The OpenLane-V2 Dataset Authors    Apache License, Version 2.0\n",
    "#\n",
    "# Contact wanghuijie@pjlab.org.cn if you have any issue.\n",
    "#\n",
    "# Copyright (c) 2023 The OpenLane-V2 Dataset Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "THICKNESS = 4\n",
    "\n",
    "COLOR_DEFAULT = (0, 0, 255)\n",
    "COLOR_DICT = {\n",
    "    0:  COLOR_DEFAULT,\n",
    "    1:  (255, 0, 0),\n",
    "    2:  (0, 255, 0),\n",
    "    3:  (255, 255, 0),\n",
    "    4:  (255, 0, 255),\n",
    "    5:  (0, 128, 128),\n",
    "    6:  (0, 128, 0),\n",
    "    7:  (128, 0, 0),\n",
    "    8:  (128, 0, 128),\n",
    "    9:  (128, 128, 0),\n",
    "    10: (0, 0, 128),\n",
    "    11: (64, 64, 64),\n",
    "    12: (192, 192, 192),\n",
    "}\n",
    "\n",
    "\n",
    "def interp_arc(points, t=1000):\n",
    "    r'''\n",
    "    Linearly interpolate equally-spaced points along a polyline, either in 2d or 3d.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : List\n",
    "        List of shape (N,2) or (N,3), representing 2d or 3d-coordinates.\n",
    "    t : array_like\n",
    "        Number of points that will be uniformly interpolated and returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like  \n",
    "        Numpy array of shape (N,2) or (N,3)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Adapted from https://github.com/johnwlambert/argoverse2-api/blob/main/src/av2/geometry/interpolate.py#L120\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # filter consecutive points with same coordinate\n",
    "    temp = []\n",
    "    for point in points:\n",
    "        point = point.tolist()\n",
    "        if temp == [] or point != temp[-1]:\n",
    "            temp.append(point)\n",
    "    if len(temp) <= 1:\n",
    "        return None\n",
    "    points = np.array(temp, dtype=points.dtype)\n",
    "\n",
    "    assert points.ndim == 2\n",
    "\n",
    "    # the number of points on the curve itself\n",
    "    n, _ = points.shape\n",
    "\n",
    "    # equally spaced in arclength -- the number of points that will be uniformly interpolated\n",
    "    eq_spaced_points = np.linspace(0, 1, t)\n",
    "\n",
    "    # Compute the chordal arclength of each segment.\n",
    "    # Compute differences between each x coord, to get the dx's\n",
    "    # Do the same to get dy's. Then the hypotenuse length is computed as a norm.\n",
    "    chordlen = np.linalg.norm(np.diff(points, axis=0), axis=1)  # type: ignore\n",
    "    # Normalize the arclengths to a unit total\n",
    "    chordlen = chordlen / np.sum(chordlen)\n",
    "    # cumulative arclength\n",
    "\n",
    "    cumarc = np.zeros(len(chordlen) + 1)\n",
    "    cumarc[1:] = np.cumsum(chordlen)\n",
    "\n",
    "    # which interval did each point fall in, in terms of eq_spaced_points? (bin index)\n",
    "    tbins = np.digitize(eq_spaced_points, bins=cumarc).astype(int)  # type: ignore\n",
    "\n",
    "    # #catch any problems at the ends\n",
    "    tbins[np.where((tbins <= 0) | (eq_spaced_points <= 0))] = 1  # type: ignore\n",
    "    tbins[np.where((tbins >= n) | (eq_spaced_points >= 1))] = n - 1\n",
    "\n",
    "    s = np.divide((eq_spaced_points - cumarc[tbins - 1]), chordlen[tbins - 1])\n",
    "    anchors = points[tbins - 1, :]\n",
    "    # broadcast to scale each row of `points` by a different row of s\n",
    "    offsets = (points[tbins, :] - points[tbins - 1, :]) * s.reshape(-1, 1)\n",
    "    points_interp = anchors + offsets\n",
    "#     print(f\"points interp : {points_interp}\")\n",
    "    return points_interp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1739640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _project(points, intrinsic, extrinsic):\n",
    "    if points is None:\n",
    "        return points\n",
    "    \n",
    "    points_in_cam_cor = np.linalg.pinv(np.array(extrinsic['rotation'])) \\\n",
    "        @ (points.T - np.array(extrinsic['translation']).reshape(3, -1))\n",
    "    \n",
    "#     print(\"points cam 1 : \", points_in_cam_cor)\n",
    "    \n",
    "    points_in_cam_cor = points_in_cam_cor[:, points_in_cam_cor[2, :] > 0]\n",
    "#     print(\"points cam : \", points_in_cam_cor)\n",
    "    \n",
    "    if points_in_cam_cor.shape[1] > 1:\n",
    "        points_on_image_cor = np.array(intrinsic['K']) @ points_in_cam_cor\n",
    "        points_on_image_cor = points_on_image_cor / (points_on_image_cor[-1, :].reshape(1, -1))\n",
    "        points_on_image_cor = points_on_image_cor[:2, :].T\n",
    "    else:\n",
    "        points_on_image_cor = None\n",
    "#     print(\" points_on_image_cor : \", points_on_image_cor)\n",
    "    return points_on_image_cor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666c3ba",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b342517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Lane3DDataset(Dataset):\n",
    "    def __init__(self, image_dir, data_dict, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = self.load_annotations(data_dict)\n",
    "\n",
    "    def load_annotations(self, data_dict):\n",
    "        annotations = []\n",
    "        max_size = 0\n",
    "        try:\n",
    "            for ann in data_dict:\n",
    "                # print(data_dict[ann].keys())\n",
    "                frame = data_dict[ann]['sensor']['ring_front_center']\n",
    "                image_name = frame['image_path']\n",
    "                # print(image_name)\n",
    "                intrinsics = frame['intrinsic']\n",
    "                extrinsics = frame['extrinsic']\n",
    "                lane_points_3d = data_dict[ann]['annotation']['lane_segment'][0]['centerline']\n",
    "                lane_points_2d = _project(interp_arc(np.array(lane_points_3d)), frame['intrinsic'],frame['extrinsic'])\n",
    "                \n",
    "                if type(lane_points_2d) == np.ndarray:\n",
    "                    rows_to_add = 1000 - lane_points_2d.shape[0]\n",
    "                    if rows_to_add > 0:\n",
    "                        # Pad the array with zeros (or any other value you prefer) at the bottom\n",
    "                        lane_points_2d = np.pad(lane_points_2d, ((0, rows_to_add), (0, 0)), mode='constant', constant_values=0)\n",
    "                        \n",
    "                    annotations.append((image_name, intrinsics, extrinsics, lane_points_2d))\n",
    "                    if max_size < len(lane_points_2d):\n",
    "                        max_size = len(lane_points_2d)\n",
    "            print(f\"len of annotations : {len(annotations)}\")\n",
    "            print(\"max size : \", max_size)\n",
    "            return annotations\n",
    "        except:\n",
    "            print(data_dict[ann].keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, intrinsics, extrinsics, lane_points_2d = self.annotations[idx]\n",
    "        img_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        lane_points_2d = torch.tensor(lane_points_2d, dtype=torch.float32)\n",
    "\n",
    "        return image, intrinsics, extrinsics, lane_points_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37638a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of annotations : 13603\n",
      "max size :  1000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = Lane3DDataset(image_dir='../repo/OpenLane-V2/data/OpenLane-V2/', data_dict=data_dict_train, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb124de8",
   "metadata": {},
   "source": [
    "### 3D Lane Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255cb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 50\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8811b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet3DLaneDetection(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(ResNet3DLaneDetection, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the fully connected layer\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "        \n",
    "        # Add a global average pooling layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Add a fully connected layer to predict 3D lane points\n",
    "        self.fc = nn.Linear(2048, num_points * 2)  # num_points * 3 for (x, y, z) coordinates\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), -1, 2)  # Reshape to (batch_size, num_points, 3)\n",
    "\n",
    "# Number of points to detect per lane\n",
    "num_points = 1000\n",
    "model = ResNet3DLaneDetection(num_points=num_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55aef4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # Suitable for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b32488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132991e2",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1956dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunigalharish.a/.local/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model\n",
    "num_points = 1000\n",
    "model = ResNet3DLaneDetection(num_points=num_points)\n",
    "model = model.cuda()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, intrinsics, extrinsics, lane_points_2d in dataloader:\n",
    "        images = images.cuda()\n",
    "        lane_points_2d = lane_points_2d.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, lane_points_2d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'lane_detection_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ded3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d99556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b21ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97f039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
